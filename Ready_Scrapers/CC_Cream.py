import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import os  # Dosya varlƒ±ƒüƒ±nƒ± kontrol etmek i√ßin os mod√ºl√ºn√º ekledik

BASE_URL = "https://www.ewg.org/skindeep"
CATEGORIES = [
    # {"name": "Anti-aging", "product_count": 500},# 1 ki≈üi
    # {"name": "Around-eye_cream", "product_count": 1000}, # 1 ki≈üi
    # {"name": "BB_Cream", "product_count": 250}, #1 ki≈üi
    {"name": "CC_Cream", "product_count": 250}, #1 ki≈üi
    # {"name": "Facial_cleanser", "product_count": 3200}, # 3 ki≈üi
    # {"name": "Facial_moisturizer__treatment", "product_count": 5000},  # 5 ki≈üi
    # {"name": "Makeup_remover", "product_count": 600}, # 1 ki≈üi
    # {"name": "Mask", "product_count": 3000}, # 3 ki≈üi
    # {"name": "Oil_controller", "product_count": 50}, # 1 ki≈üi
    # {"name": "Pore_strips", "product_count": 60},   # 1 ki≈üi
    # {"name": "Serums_&_Essences", "product_count": 2500}, # 2 ki≈üi
    # {"name": "Skin_fading__lightener", "product_count": 50}, # 1 ki≈üi
    # {"name": "Toners__astringents", "product_count": 1000} # 1 ki≈üi
]

# Selenium tarayƒ±cƒ± ba≈ülat - ULTRA AGGRESSIVE MODE
options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

# üöÄ PERFORMANS BOOST: Gereksiz y√ºkleri kapat
options.add_argument('--disable-blink-features=AutomationControlled')
options.add_argument('--disable-extensions')
options.add_argument('--disable-plugins')
options.add_argument('--disable-images')  # RESƒ∞MLERƒ∞ Y√úKLEME
options.add_argument('--blink-settings=imagesEnabled=false')
options.add_experimental_option("prefs", {
    "profile.managed_default_content_settings.images": 2,  # Resimleri engelle
    "profile.default_content_setting_values.notifications": 2,  # Bildirimleri engelle
    "profile.managed_default_content_settings.stylesheets": 2,  # CSS'i engelle (opsiyonel)
})

# üß† MEMORY OPTIMIZATION
options.add_argument('--disable-gpu')
options.add_argument('--disable-software-rasterizer')
options.add_argument('--disable-background-networking')
options.add_argument('--disable-default-apps')
options.add_argument('--disable-sync')
options.add_argument('--metrics-recording-only')
options.add_argument('--mute-audio')

# ‚ö° PAGE LOAD STRATEGY: Normal yerine 'eager' kullan (DOM ready olunca devam et)
options.page_load_strategy = 'eager'  # 'normal' yerine 'eager' - DOM ready'de devam eder

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# ‚è±Ô∏è ULTRA FAST MODE: Minimum timeout (emergency stop var artƒ±k)
driver.set_page_load_timeout(20)  # Emergency stop var, 20 saniye yeter
driver.implicitly_wait(3)  # 3 saniye yeterli


def get_product_links_batch(category_slug, start_page, page_batch_size=10, already_collected=0, limit=250):
    """Belirli sayƒ±da sayfayƒ± tarayƒ±p URL'leri toplar"""
    product_links = []
    page = start_page
    max_page = start_page + page_batch_size

    while page < max_page and (already_collected + len(product_links)) < limit:
        # Sayfa numarasƒ± ile URL olu≈ütur
        category_display = category_slug.replace("_", " ").replace("  ", "/")
        url = f"{BASE_URL}/browse/category/{category_slug}/?category={category_display}&page={page}"

        print(f"  Sayfa {page} taranƒ±yor...")
        
        # Retry mekanizmasƒ± ile sayfa y√ºkleme
        page_loaded = False
        for retry_attempt in range(2):  # 2 deneme yeter (hƒ±z i√ßin)
            try:
                driver.get(url)
                time.sleep(0.8)  # ‚ö° 2.5 ‚Üí 0.8 saniye (eager mode var)
                page_loaded = True
                break
            except Exception as e:
                if retry_attempt < 1:  # Son denemede deƒüilse
                    print(f"  ‚ö†Ô∏è Sayfa {page} y√ºkleme hatasƒ± (Deneme {retry_attempt + 1}/2), tekrar deneniyor...")
                    time.sleep(2)  # ‚ö° 5 ‚Üí 2 saniye
                else:
                    print(f"  ‚ùå Sayfa {page} 2 denemede de y√ºklenemedi, atlanƒ±yor: {str(e)[:100]}")
        
        # Sayfa y√ºklenemediyse bir sonraki sayfaya ge√ß
        if not page_loaded:
            page += 1
            continue

        products = driver.find_elements(By.CSS_SELECTOR, "a[href*='/skindeep/products/']")

        # Eƒüer sayfada √ºr√ºn yoksa, son sayfaya ula≈ümƒ±≈üƒ±z demektir
        if not products:
            print(f"  Sayfa {page}'de √ºr√ºn bulunamadƒ±. Batch taramasƒ± tamamlandƒ±.")
            break

        page_product_count = 0
        for elem in products:
            href = elem.get_attribute("href")
            if href and "/skindeep/products/" in href and href not in product_links:
                product_links.append(href)
                page_product_count += 1
                if (already_collected + len(product_links)) >= limit:
                    break

        print(f"  Sayfa {page}'den {page_product_count} √ºr√ºn bulundu (Batch Toplam: {len(product_links)})")

        # Eƒüer bu sayfadan yeni √ºr√ºn eklenmediyse, d√∂ng√ºden √ßƒ±k
        if page_product_count == 0:
            print(f"  Sayfa {page}'de yeni √ºr√ºn bulunamadƒ±. Batch taramasƒ± tamamlandƒ±.")
            break

        page += 1

    return product_links, page


def scrape_product(url, retries=2):
    """ULTRA FAST MODE: Minimum bekleme, emergency stop"""
    for attempt in range(1, retries + 1):
        try:
            # üö® CRITICAL FIX: Page load timeout'tan √∂nce emergency stop
            # Timeout'u ge√ßici olarak 10 saniyeye d√º≈ü√ºr
            driver.set_page_load_timeout(10)
            
            try:
                driver.get(url)
                time.sleep(0.3)  # DOM parse i√ßin minimal bekleme
            except Exception:
                # Timeout alƒ±ndƒ±ysa, y√ºklemeyi durdur ve mevcut HTML'i al
                try:
                    driver.execute_script("window.stop();")
                    print(f"  üõë Timeout! Sayfa zorla durduruldu: {url[:80]}...")
                    time.sleep(0.5)
                except Exception:
                    pass
            
            # Timeout'u geri y√ºkselt
            driver.set_page_load_timeout(20)
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')

            # ‚úÖ √úr√ºn adƒ±: T√ºm h1'leri √ßek, ikincisini al (ilk "Advanced Search" oluyor)
            h1_elems = soup.select("h1")
            if len(h1_elems) > 1:
                name_elem = h1_elems[1]  # ƒ∞kinci h1 ger√ßek √ºr√ºn adƒ±
            else:
                name_elem = soup.select_one("h1")  # Eƒüer sadece bir tane varsa onu al
            if not name_elem:
                name_elem = soup.select_one("h2")  # Fallback h2

            if name_elem:
                name_text = name_elem.get_text(strip=True)
                # "## " ile ba≈ülƒ±yorsa temizle
                if name_text.startswith("##"):
                    name_text = name_text.lstrip("# ").strip()
                name = name_text
            else:
                name = "Bilinmeyen √úr√ºn"

            # Ingredients from label or packaging (genel arama i√ßin 'Ingredients from' kullan)
            ingredients_text = "Bilinmeyen Bile≈üenler"
            ingredients_elem = soup.find(string=lambda x: x and 'Ingredients from' in x)
            if ingredients_elem:
                next_elem = ingredients_elem.find_parent().find_next_sibling()
                if next_elem:
                    ingredients_text = next_elem.get_text(" ", strip=True).replace("\n", " ").strip()  # Ekstra temizlik

            return {
                'name': name,
                'ingredients': ingredients_text
            }
        except Exception as e:
            print(f"‚ö° Deneme {attempt}/{retries} - Hata {url}: {str(e)[:100]}")
            if attempt < retries:
                time.sleep(2)  # ‚ö° 5 ‚Üí 2 saniye (hƒ±z i√ßin)
            continue
    return {'name': None, 'ingredients': None}


def restart_driver():
    """Driver'ƒ± yeniden ba≈ülat - memory leak'i √∂nlemek i√ßin"""
    global driver
    try:
        driver.quit()
    except Exception:
        pass
    
    # Yeni driver olu≈ütur
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    options.add_argument('--disable-images')
    options.add_argument('--blink-settings=imagesEnabled=false')
    options.add_experimental_option("prefs", {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.notifications": 2,
        "profile.managed_default_content_settings.stylesheets": 2,
    })
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-software-rasterizer')
    options.add_argument('--disable-background-networking')
    options.add_argument('--disable-default-apps')
    options.add_argument('--disable-sync')
    options.add_argument('--metrics-recording-only')
    options.add_argument('--mute-audio')
    options.page_load_strategy = 'eager'
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.set_page_load_timeout(20)  # ‚ö° 30 ‚Üí 20
    driver.implicitly_wait(3)  # ‚ö° 8 ‚Üí 3
    
    return driver


if __name__ == "__main__":
    # üîß FIX: CSV dosyasƒ±nƒ± script ile aynƒ± dizinde olu≈ütur
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_file = os.path.join(script_dir, "CC_Cream.csv")
    
    PAGE_BATCH_SIZE = 15  # ‚ö° 10 ‚Üí 15 sayfa (daha az restart = daha hƒ±zlƒ±)
    
    # ‚úÖ RESUME CAPABILITY: Eƒüer CSV varsa, kaldƒ±ƒüƒ± yerden devam et
    id_counter = 1
    existing_urls = set()
    resume_enabled = False
    
    if os.path.exists(output_file):
        try:
            existing_df = pd.read_csv(output_file)
            if not existing_df.empty:
                id_counter = existing_df['id'].max() + 1
                existing_urls = set(existing_df['product_url'].tolist())
                resume_enabled = True
                print("\n" + "="*70)
                print("üìÇ RESUME MODE AKTIF!")
                print(f"   Mevcut CSV: {output_file}")
                print(f"   Toplam √ºr√ºn: {len(existing_df)}")
                print(f"   Son ID: {id_counter - 1}")
                print(f"   Yeni ID: {id_counter} (devam edecek)")
                print(f"   {len(existing_urls)} URL duplicate kontrol√ºnde")
                print("="*70 + "\n")
        except Exception as e:
            print(f"‚ö†Ô∏è CSV okuma hatasƒ±: {e}. Sƒ±fƒ±rdan ba≈ülanƒ±yor.")
            resume_enabled = False
    else:
        print(f"\nüìù Yeni CSV olu≈üturulacak: {output_file}\n")

    for category in CATEGORIES:
        category_name = category["name"]
        product_count = category["product_count"]
        print(f"Kategori i≈üleniyor: {category_name} ({product_count} √ºr√ºn)")

        current_page = 1
        
        # üîß RESUME: Eƒüer zaten √ºr√ºn varsa, total_collected'ƒ± ayarla
        if resume_enabled and len(existing_urls) > 0:
            total_collected = len(existing_urls)
            print(f"üîÑ Resume Mode: Mevcut {total_collected} √ºr√ºn, hedef {product_count} √ºr√ºn")
        else:
            total_collected = 0
        
        # Batch batch i≈üle: her seferde PAGE_BATCH_SIZE sayfa tara, scrape et, kaydet
        while total_collected < product_count:
            print(f"\n‚ö° ULTRA FAST BATCH: Sayfa {current_page} - {current_page + PAGE_BATCH_SIZE - 1} ‚ö°")
            
            # Bu batch'teki URL'leri topla
            product_urls, next_page = get_product_links_batch(
                category_name, 
                start_page=current_page, 
                page_batch_size=PAGE_BATCH_SIZE,
                already_collected=total_collected,
                limit=product_count
            )
            
            # Eƒüer URL bulunamadƒ±ysa, i≈ülem tamamlanmƒ±≈ü demektir
            if not product_urls:
                print("Daha fazla √ºr√ºn bulunamadƒ±. Kategori tamamlandƒ±.")
                break
            
            # Bu batch'teki URL'leri hemen scrape et ve kaydet
            skipped_count = 0
            for url in product_urls:
                # üîç Duplicate kontrol√º: Bu URL zaten scrape edildiyse atla
                if url in existing_urls:
                    skipped_count += 1
                    continue  # Sayƒ±ma dahil etme, sadece skip et
                
                product_data = scrape_product(url)

                # ‚ùå Eƒüer √ºr√ºn bilgisi alƒ±namadƒ±ysa (name=None), atla ve kaydetme
                if product_data['name'] is None:
                    print(f"‚ö†Ô∏è ATLANDI (Hatalƒ± √ºr√ºn): {url}")
                    continue

                # Anlƒ±k olarak DataFrame olu≈ütur
                df_row = pd.DataFrame([{
                    "id": id_counter,
                    "category": category_name,
                    # "product_count": product_count,
                    "product_url": url,
                    "name": product_data['name'],
                    "ingredients": product_data['ingredients']
                }])

                # Dosyanƒ±n var olup olmadƒ±ƒüƒ±nƒ± kontrol ederek ba≈ülƒ±k (header) eklenip eklenmeyeceƒüine karar ver
                # Dosya yoksa header=True, varsa header=False olacak
                header = not os.path.exists(output_file)

                # 'append' modunda ('a') dosyaya ekle
                df_row.to_csv(output_file, mode='a', header=header, index=False, encoding="utf-8")

                print(f"Kaydedildi: ID {id_counter} - {product_data.get('name', 'Hata!')}")

                # URL'i hafƒ±zaya ekle (duplicate √∂nlemi)
                existing_urls.add(url)
                
                id_counter += 1
                total_collected += 1
                
                # Eƒüer limit'e ula≈ütƒ±ysak dur
                if total_collected >= product_count:
                    break
            
            # Bir sonraki batch i√ßin sayfa numarasƒ±nƒ± g√ºncelle
            current_page = next_page
            
            # Batch √∂zeti
            if skipped_count > 0:
                print(f"\n‚úÖ Batch tamamlandƒ±! {total_collected}/{product_count} √ºr√ºn | ‚è≠Ô∏è {skipped_count} duplicate skip")
            else:
                print(f"\n‚úÖ Batch tamamlandƒ±! {total_collected}/{product_count} √ºr√ºn")
            
            # üîÑ HER BATCH SONRASI DRIVER'I RESTART ET (Memory leak √∂nlemi)
            if total_collected < product_count:
                print("üîÑ Driver yeniden ba≈ülatƒ±lƒ±yor (memory temizliƒüi)...")
                driver = restart_driver()
                time.sleep(0.5)  # ‚ö° 2 ‚Üí 0.5 saniye

    # Tarayƒ±cƒ±yƒ± kapat
    driver.quit()

    print(f"\nScraping tamamlandƒ±. Sonu√ß {output_file} dosyasƒ±na yazƒ±ldƒ±.")

    # Sonucun √∂nizlemesini g√∂stermek i√ßin dosyayƒ± oku
    try:
        final_df = pd.read_csv(output_file)
        print("Veri √∂nizlemesi:")
        print(final_df.head())
    except pd.errors.EmptyDataError:
        print("CSV dosyasƒ± bo≈ü, hi√ßbir veri kazƒ±namadƒ±.")