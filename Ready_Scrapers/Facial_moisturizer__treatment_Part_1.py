import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import os  # Dosya varlÄ±ÄŸÄ±nÄ± kontrol etmek iÃ§in os modÃ¼lÃ¼nÃ¼ ekledik

BASE_URL = "https://www.ewg.org/skindeep"
CATEGORIES = [
    # {"name": "Anti-aging", "product_count": 500},# 1 kiÅŸi
    # {"name": "Around-eye_cream", "product_count": 1000}, # 1 kiÅŸi
    # {"name": "BB_Cream", "product_count": 250}, #1 kiÅŸi
    # {"name": "CC_Cream", "product_count": 250}, #1 kiÅŸi
    # {"name": "Facial_cleanser", "product_count": 3200},  # 3 kiÅŸi
    {"name": "Facial_moisturizer__treatment", "product_count": 5000},  # 5 kiÅŸi
    # {"name": "Makeup_remover", "product_count": 600}, # 1 kiÅŸi
    # {"name": "Mask", "product_count": 3000}, # 3 kiÅŸi
    # {"name": "Oil_controller", "product_count": 50}, # 1 kiÅŸi
    # {"name": "Pore_strips", "product_count": 60},   # 1 kiÅŸi
    # {"name": "Serums_&_Essences", "product_count": 2500}, # 2 kiÅŸi
    # {"name": "Skin_fading__lightener", "product_count": 50}, # 1 kiÅŸi
    # {"name": "Toners__astringents", "product_count": 1000} # 1 kiÅŸi
]

# Selenium tarayÄ±cÄ± baÅŸlat
options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# â±ï¸ HARD MODE: Agresif timeout ayarlarÄ±
driver.set_page_load_timeout(15)
driver.implicitly_wait(5)


def get_product_links_batch(category_slug, start_page, page_batch_size=10, already_collected=0, limit=3200):
    """Belirli sayÄ±da sayfayÄ± tarayÄ±p URL'leri toplar"""
    product_links = []
    page = start_page
    max_page = start_page + page_batch_size

    while page < max_page and (already_collected + len(product_links)) < limit:
        # Sayfa numarasÄ± ile URL oluÅŸtur
        category_display = category_slug.replace("_", " ").replace("  ", "/")
        url = f"{BASE_URL}/browse/category/{category_slug}/?category={category_display}&page={page}"

        print(f"  Sayfa {page} taranÄ±yor...")
        try:
            driver.get(url)
            time.sleep(1.5)
        except Exception as e:
            print(f"  âš ï¸ Sayfa {page} yÃ¼klenemedi, atlanÄ±yor: {e}")
            break

        products = driver.find_elements(By.CSS_SELECTOR, "a[href*='/skindeep/products/']")

        # EÄŸer sayfada Ã¼rÃ¼n yoksa, son sayfaya ulaÅŸmÄ±ÅŸÄ±z demektir
        if not products:
            print(f"  Sayfa {page}'de Ã¼rÃ¼n bulunamadÄ±. Batch taramasÄ± tamamlandÄ±.")
            break

        page_product_count = 0
        for elem in products:
            href = elem.get_attribute("href")
            if href and "/skindeep/products/" in href and href not in product_links:
                product_links.append(href)
                page_product_count += 1
                if (already_collected + len(product_links)) >= limit:
                    break

        print(f"  Sayfa {page}'den {page_product_count} Ã¼rÃ¼n bulundu (Batch Toplam: {len(product_links)})")

        # EÄŸer bu sayfadan yeni Ã¼rÃ¼n eklenmediyse, dÃ¶ngÃ¼den Ã§Ä±k
        if page_product_count == 0:
            print(f"  Sayfa {page}'de yeni Ã¼rÃ¼n bulunamadÄ±. Batch taramasÄ± tamamlandÄ±.")
            break

        page += 1

    return product_links, page


def scrape_product(url, retries=2):
    """HARD MODE: HÄ±zlÄ± scraping, minimal bekleme"""
    for attempt in range(1, retries + 1):
        try:
            driver.get(url)
            WebDriverWait(driver, 8).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            time.sleep(2)  # sayfa tam yÃ¼klenmesi iÃ§in
            soup = BeautifulSoup(driver.page_source, 'html.parser')

            # âœ… ÃœrÃ¼n adÄ±: TÃ¼m h1'leri Ã§ek, ikincisini al (ilk "Advanced Search" oluyor)
            h1_elems = soup.select("h1")
            if len(h1_elems) > 1:
                name_elem = h1_elems[1]  # Ä°kinci h1 gerÃ§ek Ã¼rÃ¼n adÄ±
            else:
                name_elem = soup.select_one("h1")  # EÄŸer sadece bir tane varsa onu al
            if not name_elem:
                name_elem = soup.select_one("h2")  # Fallback h2

            if name_elem:
                name_text = name_elem.get_text(strip=True)
                # "## " ile baÅŸlÄ±yorsa temizle
                if name_text.startswith("##"):
                    name_text = name_text.lstrip("# ").strip()
                name = name_text
            else:
                name = "Bilinmeyen ÃœrÃ¼n"

            # Ingredients from label or packaging (genel arama iÃ§in 'Ingredients from' kullan)
            ingredients_text = "Bilinmeyen BileÅŸenler"
            ingredients_elem = soup.find(string=lambda x: x and 'Ingredients from' in x)
            if ingredients_elem:
                next_elem = ingredients_elem.find_parent().find_next_sibling()
                if next_elem:
                    ingredients_text = next_elem.get_text(" ", strip=True).replace("\n", " ").strip()  # Ekstra temizlik

            return {
                'name': name,
                'ingredients': ingredients_text
            }
        except Exception as e:
            print(f"âš¡ Deneme {attempt}/{retries} - Hata {url}: {str(e)[:100]}")
            if attempt < retries:
                time.sleep(3)  # Retry Ã¶ncesi bekle
            continue
    return {'name': None, 'ingredients': None}


if __name__ == "__main__":
    # ==================== AYARLAR ====================
    # Her kiÅŸi iÃ§in farklÄ± deÄŸerler:
    # 1. KiÅŸi: START_PAGE = 1,   PERSON_SUFFIX = "part1"
    # 2. KiÅŸi: START_PAGE = 100, PERSON_SUFFIX = "part2"
    # 3. KiÅŸi: START_PAGE = 200, PERSON_SUFFIX = "part3"

    START_PAGE = 1  # BaÅŸlangÄ±Ã§ sayfasÄ± (1, 100, 200, vb.)
    PERSON_SUFFIX = "part1"  # Dosya adÄ± iÃ§in ek (part1, part2, part3, vb.)
    PRODUCT_LIMIT = 1000  # Her kiÅŸinin Ã§ekeceÄŸi Ã¼rÃ¼n sayÄ±sÄ± (3200 / 3 â‰ˆ 1067, gÃ¼venlik iÃ§in 1100)
    PAGE_BATCH_SIZE = 10  # Her seferde 10 sayfa iÅŸle
    # ================================================

    output_file = f"Facial_moisturizer__treatment_{PERSON_SUFFIX}.csv"
    
    # ğŸ”„ KALDÄI YERDEN DEVAM ETME: CSV'yi kontrol et
    id_counter = 1
    already_scraped = 0
    scraped_urls = set()
    
    if os.path.exists(output_file):
        try:
            existing_df = pd.read_csv(output_file)
            already_scraped = len(existing_df)
            id_counter = already_scraped + 1
            
            if 'product_url' in existing_df.columns:
                scraped_urls = set(existing_df['product_url'].dropna().tolist())
            
            print(f"ğŸ“ Mevcut CSV bulundu: {already_scraped} Ã¼rÃ¼n zaten Ã§ekilmiÅŸ.")
            
            # EÄŸer hedef tamamlandÄ±ysa Ã§Ä±k
            if already_scraped >= PRODUCT_LIMIT:
                print(f"âœ… Hedef tamamlandÄ±! ({already_scraped}/{PRODUCT_LIMIT}) Script sonlandÄ±rÄ±lÄ±yor.")
                driver.quit()
                exit()
        except Exception as e:
            print(f"âš ï¸ CSV okuma hatasÄ±: {e}, baÅŸtan baÅŸlanÄ±yor...")
            already_scraped = 0
            id_counter = 1
            scraped_urls = set()

    for category in CATEGORIES:
        category_name = category["name"]
        product_count = category["product_count"]
        print(f"Kategori iÅŸleniyor: {category_name} ({product_count} Ã¼rÃ¼n)")
        print(f"BaÅŸlangÄ±Ã§ sayfasÄ±: {START_PAGE}, Hedef Ã¼rÃ¼n sayÄ±sÄ±: {PRODUCT_LIMIT}")

        current_page = START_PAGE
        total_collected = already_scraped  # KaldÄ±ÄŸÄ± yerden baÅŸla
        
        # Batch batch iÅŸle: her seferde PAGE_BATCH_SIZE sayfa tara, scrape et, kaydet
        while total_collected < PRODUCT_LIMIT:
            print(f"\n--- Batch iÅŸleniyor: Sayfa {current_page} - {current_page + PAGE_BATCH_SIZE - 1} ---")
            
            # Bu batch'teki URL'leri topla
            product_urls, next_page = get_product_links_batch(
                category_name, 
                start_page=current_page, 
                page_batch_size=PAGE_BATCH_SIZE,
                already_collected=total_collected,
                limit=PRODUCT_LIMIT
            )
            
            # EÄŸer URL bulunamadÄ±ysa, iÅŸlem tamamlanmÄ±ÅŸ demektir
            if not product_urls:
                print("Daha fazla Ã¼rÃ¼n bulunamadÄ±. Kategori tamamlandÄ±.")
                break
            
            # Bu batch'teki URL'leri hemen scrape et ve kaydet
            for url in product_urls:
                if url in scraped_urls:
                    print(f"â­ï¸ Zaten Ã§ekilmiÅŸ, atlanÄ±yor: {url}")
                    continue
                
                product_data = scrape_product(url)

                if product_data['name'] is None:
                    print(f"âš ï¸ ATLANDI (HatalÄ± Ã¼rÃ¼n): {url}")
                    continue

                # AnlÄ±k olarak DataFrame oluÅŸtur
                df_row = pd.DataFrame([{
                    "id": id_counter,
                    "category": category_name,
                    # "product_count": product_count,
                    "product_url": url,
                    "name": product_data['name'],
                    "ingredients": product_data['ingredients']
                }])

                # DosyanÄ±n var olup olmadÄ±ÄŸÄ±nÄ± kontrol ederek baÅŸlÄ±k (header) eklenip eklenmeyeceÄŸine karar ver
                # Dosya yoksa header=True, varsa header=False olacak
                header = not os.path.exists(output_file)

                # 'append' modunda ('a') dosyaya ekle
                df_row.to_csv(output_file, mode='a', header=header, index=False, encoding="utf-8")

                print(f"Kaydedildi: ID {id_counter} - {product_data.get('name', 'Hata!')}")

                id_counter += 1
                total_collected += 1
                
                # EÄŸer limit'e ulaÅŸtÄ±ysak dur
                if total_collected >= PRODUCT_LIMIT:
                    break
            
            # Bir sonraki batch iÃ§in sayfa numarasÄ±nÄ± gÃ¼ncelle
            current_page = next_page
            
            print(f"Batch tamamlandÄ±. Toplam {total_collected}/{PRODUCT_LIMIT} Ã¼rÃ¼n toplandÄ±.")

    # TarayÄ±cÄ±yÄ± kapat
    driver.quit()

    print(f"\nScraping tamamlandÄ±. SonuÃ§ {output_file} dosyasÄ±na yazÄ±ldÄ±.")

    # Sonucun Ã¶nizlemesini gÃ¶stermek iÃ§in dosyayÄ± oku
    try:
        final_df = pd.read_csv(output_file)
        print("Veri Ã¶nizlemesi:")
        print(final_df.head())
    except pd.errors.EmptyDataError:
        print("CSV dosyasÄ± boÅŸ, hiÃ§bir veri kazÄ±namadÄ±.")